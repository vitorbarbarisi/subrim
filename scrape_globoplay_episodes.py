#!/usr/bin/env python3
"""
Script para extrair todos os epis√≥dios do Globo Play usando Selenium
Este script simula o scroll infinito para carregar todos os epis√≥dios dinamicamente
"""

import time
import json
import csv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GloboPlayScraper:
    def __init__(self, url="https://globoplay.globo.com/v/13385190/?s=0s", headless=False):
        self.url = url
        self.episodes = []
        self.driver = None
        self.headless = headless
        self.json_filename = "globoplay_episodes.json"
        self.csv_filename = "globoplay_episodes.csv"

    def setup_driver(self):
        """Configura o ChromeDriver com op√ß√µes para simular navegador real"""
        chrome_options = Options()

        # Modo headless (opcional)
        if self.headless:
            chrome_options.add_argument("--headless")

        # Op√ß√µes b√°sicas
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")

        # User agent mais realista
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        # Op√ß√µes para evitar detec√ß√£o de headless
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        # Simular navegador mais real
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--allow-running-insecure-content")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # Carrega mais r√°pido

        try:
            self.driver = webdriver.Chrome(options=chrome_options)

            # Remove webdriver property para evitar detec√ß√£o
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            logger.info("ChromeDriver inicializado com sucesso (modo stealth)")
        except Exception as e:
            logger.error(f"Erro ao inicializar ChromeDriver: {e}")
            raise

    def wait_for_page_load(self, timeout=30):
        """Espera a p√°gina carregar completamente"""
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            logger.info("P√°gina carregada completamente")
        except TimeoutException:
            logger.warning("Timeout ao esperar p√°gina carregar")

    def scroll_to_bottom(self, scroll_pause_time=3, max_scrolls=100):
        """Simula scroll infinito para carregar todos os epis√≥dios com m√∫ltiplas estrat√©gias"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scrolls = 0
        no_change_count = 0
        max_no_change = 5  # M√°ximo de scrolls sem mudan√ßa antes de parar

        # Ajusta comportamento baseado no modo (visual vs headless)
        if not self.headless:
            # Modo visual: mais lento e interativo
            scroll_pause_time = 8  # Mais tempo para o usu√°rio
            max_scrolls = 20  # Menos scrolls autom√°ticos
            logger.info("üîÑ MODO VISUAL: Scroll mais lento para permitir intera√ß√£o do usu√°rio")
        else:
            logger.info("ü§ñ MODO HEADLESS: Scroll autom√°tico otimizado")

        logger.info(f"Iniciando scroll - Altura inicial: {last_height}px")

        while scrolls < max_scrolls and no_change_count < max_no_change:
            # Estrat√©gia 1: Scroll gradual (mais natural)
            current_position = self.driver.execute_script("return window.pageYOffset")
            scroll_amount = 800  # Scroll de 800px por vez
            new_position = current_position + scroll_amount

            self.driver.execute_script(f"window.scrollTo(0, {new_position});")
            logger.info(f"Scroll gradual para posi√ß√£o: {new_position}px")

            # Espera conte√∫do carregar
            time.sleep(scroll_pause_time)

            # Estrat√©gia 2: Scroll completo para o final (backup)
            if scrolls % 3 == 0:  # A cada 3 scrolls, faz um scroll completo
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)

            # Calcula nova altura da p√°gina
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            current_height = self.driver.execute_script("return window.innerHeight + window.pageYOffset")

            # Conta epis√≥dios atuais para verificar se novos foram carregados
            try:
                current_episodes = len(self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/v/'][href*='?s=0s']"))
                logger.info(f"Epis√≥dios encontrados at√© agora: {current_episodes}")
            except:
                current_episodes = 0

            logger.info(f"Scroll {scrolls+1}/{max_scrolls} - Altura: {new_height}px - Posi√ß√£o atual: {current_height}px - Epis√≥dios: {current_episodes}")

            if new_height == last_height:
                no_change_count += 1
                logger.info(f"Sem mudan√ßa na altura ({no_change_count}/{max_no_change})")
            else:
                no_change_count = 0  # Reset counter se houve mudan√ßa
                logger.info(f"Altura mudou! Anterior: {last_height}px -> Novo: {new_height}px")

            last_height = new_height
            scrolls += 1

            # Pausa extra se estamos pr√≥ximos do final
            if current_height >= new_height - 1000:
                logger.info("Pr√≥ximo do final da p√°gina")
                time.sleep(2)

        logger.info(f"Scroll conclu√≠do ap√≥s {scrolls} scrolls. Altura final: {last_height}px")
        return scrolls


    def extract_episodes(self):
        """Extrai informa√ß√µes completas dos epis√≥dios da p√°gina"""
        try:
            # Verifica se a sess√£o do navegador ainda est√° v√°lida
            try:
                self.driver.current_url
            except Exception as e:
                logger.warning("‚ö†Ô∏è  Sess√£o do navegador n√£o est√° mais dispon√≠vel")
                logger.warning("üí° Isso pode acontecer se voc√™ fechou a janela muito rapidamente")
                logger.info("üîÑ Tentando processar dados j√° coletados...")
                # Se n√£o temos dados, retornamos lista vazia
                if not self.episodes:
                    logger.info("üìù Nenhum dado foi coletado antes do fechamento da janela")
                return

            # Espera os elementos de epis√≥dio aparecerem
            WebDriverWait(self.driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "[href*='/v/']"))
            )

            # Encontra todos os links de epis√≥dios diretamente
            episode_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/v/'][href*='?s=0s']")

            logger.info(f"Encontrados {len(episode_links)} epis√≥dios para processar")

            if episode_links:
                logger.info("‚úÖ Epis√≥dios encontrados com sucesso")
            else:
                logger.warning("‚ö†Ô∏è  Nenhum epis√≥dio encontrado na p√°gina atual")
                logger.info("üí° Dica: Certifique-se de fazer scroll suficiente antes de fechar a janela")

            # Para cada link, encontra o container pai que cont√©m todas as informa√ß√µes
            episode_containers = []
            for link in episode_links:
                container_found = False
                
                # Estrat√©gia 1: Procura por elemento pai direto (li, div, article, etc) que seja um card
                try:
                    direct_parent = link.find_element(By.XPATH, "./..")
                    direct_parent_text = direct_parent.text.strip()
                    # Se o pai direto tem texto razo√°vel (entre 20 e 500 caracteres), provavelmente √© o card
                    if 20 < len(direct_parent_text) < 500:
                        episode_containers.append(direct_parent)
                        container_found = True
                except:
                    pass
                
                # Estrat√©gia 2: Se n√£o encontrou, procura por elemento com classes espec√≠ficas
                if not container_found:
                    try:
                        parent = link.find_element(By.XPATH, "./ancestor::li[1] | ./ancestor::div[contains(@class, 'card') or contains(@class, 'item') or contains(@class, 'episode')][1] | ./ancestor::article[1]")
                        parent_text = parent.text.strip()
                        if 20 < len(parent_text) < 500:
                            episode_containers.append(parent)
                            container_found = True
                    except:
                        pass
                
                # Estrat√©gia 3: Se ainda n√£o encontrou, usa o pai direto mesmo que tenha pouco texto
                if not container_found:
                    try:
                        parent = link.find_element(By.XPATH, "./..")
                        episode_containers.append(parent)
                    except:
                        # √öltimo recurso: usa o pr√≥prio link
                        episode_containers.append(link)

            for i, container in enumerate(episode_containers):
                try:
                    # Extrai informa√ß√µes do container (i+1 porque o √≠ndice come√ßa em 0)
                    episode_info = self.extract_episode_info(container, episode_index=i+1)

                    logger.debug(f"Container {i+1}: extract_episode_info retornou: {episode_info}")

                    if episode_info:
                        if episode_info not in self.episodes:  # Evita duplicatas
                            self.episodes.append(episode_info)
                            logger.debug(f"‚úÖ Epis√≥dio adicionado √† lista: {episode_info['episode_number']} - {episode_info['chapter_date']}")
                        else:
                            logger.debug(f"‚ö†Ô∏è Epis√≥dio duplicado ignorado: {episode_info['episode_number']}")
                    else:
                        logger.debug(f"‚ùå extract_episode_info retornou None para container {i+1}")

                except Exception as e:
                    logger.debug(f"Erro ao extrair epis√≥dio {i+1}: {e}")
                    continue

            logger.info(f"üéØ Ap√≥s loop de extra√ß√£o: {len(self.episodes)} epis√≥dios na lista")

            # Remove duplicatas baseadas no ID
            logger.info(f"üîÑ Iniciando deduplica√ß√£o: {len(self.episodes)} epis√≥dios antes")
            unique_episodes = []
            seen_ids = set()
            for ep in self.episodes:
                if ep['id'] not in seen_ids:
                    unique_episodes.append(ep)
                    seen_ids.add(ep['id'])
                else:
                    logger.debug(f"üóëÔ∏è Removendo duplicata: {ep['id']}")

            self.episodes = unique_episodes
            logger.info(f"‚úÖ Ap√≥s deduplica√ß√£o: {len(self.episodes)} epis√≥dios √∫nicos")

        except Exception as e:
            if "no such window" in str(e).lower():
                logger.warning("‚ö†Ô∏è  Janela do navegador foi fechada durante a extra√ß√£o")
                logger.info("üí° Dica: Da pr√≥xima vez, aguarde alguns segundos ap√≥s fazer scroll antes de fechar")
                if self.episodes:
                    logger.info(f"üìä Usando {len(self.episodes)} epis√≥dios coletados antes do fechamento")
                else:
                    logger.info("üìù Nenhum epis√≥dio foi coletado")
            else:
                logger.error(f"Erro geral na extra√ß√£o: {e}")

    def extract_episode_info(self, container, episode_index=None):
        """Extrai informa√ß√µes detalhadas de um container de epis√≥dio"""
        try:
            import re
            logger.debug("üîç Iniciando extra√ß√£o de info do container")

            # Extrai URL
            if container.tag_name == 'a':
                href = container.get_attribute("href")
                link_element = container
                logger.debug(f"URL direta do link: {href}")
            else:
                link_element = container.find_element(By.CSS_SELECTOR, "a[href*='/v/']")
                href = link_element.get_attribute("href")
                logger.debug(f"URL do elemento encontrado: {href}")

            if not href or '/v/' not in href:
                logger.debug("‚ùå URL inv√°lida ou n√£o cont√©m '/v/'")
                return None

            # Extrai ID do v√≠deo
            video_id = href.split('/v/')[1].split('/')[0]
            logger.debug(f"ID do v√≠deo extra√≠do: {video_id}")

            # Tenta extrair n√∫mero do epis√≥dio e data do cap√≠tulo
            episode_number = ""
            chapter_date = ""
            
            # Estrat√©gia 1: Extrai do atributo title do link (mais confi√°vel)
            try:
                title_attr = link_element.get_attribute("title")
                if title_attr:
                    logger.debug(f"Atributo title encontrado: '{title_attr}'")
                    # Procura por padr√£o "Cap√≠tulo de DD/MM/YYYY"
                    date_match = re.search(r'Cap√≠tulo de\s+(\d{2}/\d{2}/\d{4})', title_attr, re.IGNORECASE)
                    if date_match:
                        chapter_date = date_match.group(1)
                        logger.debug(f"Data extra√≠da do title: {chapter_date}")
            except:
                pass

            # Tenta encontrar o elemento pai que cont√©m mais informa√ß√µes
            full_text = ""
            parent_element = None
            
            # Estrat√©gia 1: Procura por elemento pai com classes espec√≠ficas
            try:
                parent_element = link_element.find_element(By.XPATH, "./ancestor::*[contains(@class, 'episode') or contains(@class, 'card') or contains(@class, 'item') or contains(@class, 'chapter') or contains(@class, 'video') or contains(@class, 'content')][1]")
                full_text = parent_element.text.strip()
                logger.debug(f"Texto extra√≠do do elemento pai (estrat√©gia 1): '{full_text[:200]}...'")
            except:
                pass
            
            # Estrat√©gia 2: Se n√£o encontrou, tenta pegar o elemento pai direto (div, article, etc)
            if not full_text:
                try:
                    parent_element = link_element.find_element(By.XPATH, "./..")
                    full_text = parent_element.text.strip()
                    logger.debug(f"Texto extra√≠do do pai direto (estrat√©gia 2): '{full_text[:200]}...'")
                except:
                    pass
            
            # Estrat√©gia 3: Se ainda n√£o encontrou, tenta pegar do container
            if not full_text:
                try:
                    full_text = container.text.strip()
                    logger.debug(f"Texto extra√≠do do container (estrat√©gia 3): '{full_text[:200]}...'")
                except:
                    pass
            
            # Estrat√©gia 4: √öltimo recurso - pega do pr√≥prio link
            if not full_text:
                try:
                    full_text = link_element.text.strip()
                    logger.debug(f"Texto extra√≠do do link (estrat√©gia 4): '{full_text[:200]}...'")
                except:
                    full_text = ""
            
            # Se ainda n√£o tem texto, tenta pegar o innerHTML para an√°lise
            if not full_text or len(full_text) < 10:
                try:
                    if parent_element:
                        html_content = parent_element.get_attribute('innerHTML')
                    else:
                        html_content = container.get_attribute('innerHTML')
                    # Extrai texto do HTML removendo tags
                    import re
                    text_from_html = re.sub(r'<[^>]+>', ' ', html_content)
                    text_from_html = ' '.join(text_from_html.split())
                    if text_from_html and len(text_from_html) > len(full_text):
                        full_text = text_from_html
                        logger.debug(f"Texto extra√≠do do HTML (estrat√©gia 5): '{full_text[:200]}...'")
                except:
                    pass

            logger.debug(f"Texto final extra√≠do: '{full_text[:300]}...'")

            # Procura por padr√£o "X. Cap√≠tulo X" ou "Epis√≥dio X"
            episode_match = re.search(r'(?:^|\s)(\d+)\.\s*(?:Cap√≠tulo|Epis√≥dio)', full_text, re.IGNORECASE | re.MULTILINE)
            if episode_match:
                episode_number = episode_match.group(1)
                logger.debug(f"N√∫mero do epis√≥dio encontrado (padr√£o X.): {episode_number}")
            else:
                # Tenta padr√£o "Epis√≥dio X"
                episode_match = re.search(r'(?:Epis√≥dio|Episodio)\s*(\d+)', full_text, re.IGNORECASE)
                if episode_match:
                    episode_number = episode_match.group(1)
                    logger.debug(f"N√∫mero do epis√≥dio encontrado (padr√£o Epis√≥dio): {episode_number}")

            # Procura por padr√£o "Cap√≠tulo de DD/MM/YYYY"
            date_match = re.search(r'Cap√≠tulo de\s+(\d{2}/\d{2}/\d{4})', full_text, re.IGNORECASE)
            if date_match:
                chapter_date = date_match.group(1)
                logger.debug(f"Data do cap√≠tulo encontrada: {chapter_date}")
            else:
                # Tenta padr√£o alternativo "DD/MM/YYYY"
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})', full_text)
                if date_match:
                    chapter_date = date_match.group(1)
                    logger.debug(f"Data encontrada (padr√£o alternativo): {chapter_date}")

            # Estrat√©gia 2: Usa JavaScript para encontrar elementos que podem conter o n√∫mero do epis√≥dio
            if not episode_number:
                try:
                    # Procura por elementos h2, h3, h4 no container usando JavaScript
                    headings_text = self.driver.execute_script("""
                        var container = arguments[0];
                        var headings = container.querySelectorAll('h1, h2, h3, h4, h5, h6');
                        var texts = [];
                        for (var i = 0; i < headings.length; i++) {
                            texts.push(headings[i].textContent.trim());
                        }
                        return texts;
                    """, container)
                    
                    for heading_text in headings_text:
                        # Procura por padr√£o "X. Cap√≠tulo X" ou "X. Epis√≥dio X"
                        episode_match = re.search(r'^(\d+)\.\s*(?:Cap√≠tulo|Epis√≥dio)', heading_text, re.IGNORECASE)
                        if episode_match:
                            episode_number = episode_match.group(1)
                            logger.debug(f"N√∫mero do epis√≥dio encontrado em heading: {episode_number}")
                            break
                except:
                    pass
            
            # Estrat√©gia 3: Procura por n√∫mero do epis√≥dio no texto completo
            if not episode_number and full_text:
                # Procura por padr√£o "X. Cap√≠tulo" no in√≠cio do texto
                episode_match = re.search(r'^(\d+)\.\s*(?:Cap√≠tulo|Epis√≥dio)', full_text, re.IGNORECASE | re.MULTILINE)
                if episode_match:
                    episode_number = episode_match.group(1)
                    logger.debug(f"N√∫mero do epis√≥dio encontrado no texto (padr√£o X.): {episode_number}")
                else:
                    # Procura por padr√£o "Cap√≠tulo X" ou "Epis√≥dio X"
                    episode_match = re.search(r'(?:Cap√≠tulo|Epis√≥dio)\s+(\d+)', full_text, re.IGNORECASE)
                    if episode_match:
                        episode_number = episode_match.group(1)
                        logger.debug(f"N√∫mero do epis√≥dio encontrado no texto (padr√£o Cap√≠tulo X): {episode_number}")
            
            # Estrat√©gia 3: Tenta buscar em elementos filhos espec√≠ficos (h2, h3, etc)
            if not episode_number:
                try:
                    # Procura por elementos h2, h3, h4 que podem conter o t√≠tulo
                    for tag in ['h2', 'h3', 'h4', 'h5']:
                        try:
                            title_elem = container.find_element(By.CSS_SELECTOR, tag)
                            title_text = title_elem.text.strip()
                            
                            episode_match = re.search(r'(?:^|\s)(\d+)\.\s*(?:Cap√≠tulo|Epis√≥dio)', title_text, re.IGNORECASE)
                            if episode_match:
                                episode_number = episode_match.group(1)
                                logger.debug(f"N√∫mero encontrado em {tag}: {episode_number}")
                                break
                        except:
                            continue
                except:
                    pass
            
            # Estrat√©gia 4: Se ainda n√£o encontrou a data, tenta do atributo alt da imagem
            if not chapter_date:
                try:
                    img_elem = container.find_element(By.CSS_SELECTOR, "img")
                    alt_text = img_elem.get_attribute("alt")
                    if alt_text:
                        date_match = re.search(r'Cap√≠tulo de\s+(\d{2}/\d{2}/\d{4})', alt_text, re.IGNORECASE)
                        if date_match:
                            chapter_date = date_match.group(1)
                            logger.debug(f"Data encontrada no alt da imagem: {chapter_date}")
                except:
                    pass

            # Fallback se n√£o conseguiu extrair: usa o √≠ndice se fornecido
            if not episode_number:
                if episode_index is not None:
                    episode_number = str(episode_index)
                    logger.debug(f"Usando √≠ndice como n√∫mero do epis√≥dio: {episode_number}")
                else:
                    episode_number = "N/A"
                    logger.debug(f"Fallback usado para epis√≥dio: {episode_number}")

            if not chapter_date:
                chapter_date = f"Cap√≠tulo {video_id}"
                logger.debug(f"Fallback usado para cap√≠tulo: {chapter_date}")

            # Cria t√≠tulo completo
            if episode_number != "N/A" and episode_number != "Atual":
                if chapter_date and chapter_date != f"Cap√≠tulo {video_id}":
                    full_title = f"Epis√≥dio {episode_number}, {chapter_date}"
                else:
                    full_title = f"Epis√≥dio {episode_number}"
            else:
                full_title = chapter_date

            result = {
                'id': video_id,
                'episode_number': episode_number,
                'chapter_date': chapter_date,
                'title': full_title,
                'url': href,
                'type': 'episode'
            }

            logger.debug(f"‚úÖ Extra√ß√£o conclu√≠da com sucesso: {result}")
            return result

        except Exception as e:
            logger.debug(f"‚ùå Erro ao extrair info do container: {e}")
            return None

    def save_to_json(self, filename=None):
        """Salva os epis√≥dios em arquivo JSON"""
        if filename is None:
            filename = self.json_filename

        try:
            logger.info(f"üíæ Salvando {len(self.episodes)} epis√≥dios em {filename}...")

            with open(filename, 'w', encoding='utf-8') as f:
                data = {
                    'metadata': {
                        'source_url': self.url,
                        'extraction_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'total_episodes': len(self.episodes)
                    },
                    'episodes': self.episodes
                }
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"‚úÖ Arquivo JSON salvo: {filename} ({len(self.episodes)} epis√≥dios)")

            # Verifica se arquivo foi criado
            import os
            if os.path.exists(filename):
                size = os.path.getsize(filename)
                logger.info(f"üìÅ Arquivo criado: {size} bytes")
            else:
                logger.error(f"‚ùå Arquivo n√£o foi criado: {filename}")

        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar JSON: {e}")
            raise

    def save_to_csv(self, filename=None):
        """Salva os epis√≥dios em arquivo CSV com informa√ß√µes completas"""
        if filename is None:
            filename = self.csv_filename

        try:
            logger.info(f"üíæ Salvando {len(self.episodes)} epis√≥dios em {filename}...")

            if self.episodes:
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    # Define as colunas na ordem desejada
                    fieldnames = ['episode_number', 'chapter_date', 'id', 'title', 'url', 'type']
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(self.episodes)

                logger.info(f"‚úÖ Arquivo CSV salvo: {filename} ({len(self.episodes)} epis√≥dios)")

                # Verifica se arquivo foi criado
                import os
                if os.path.exists(filename):
                    size = os.path.getsize(filename)
                    logger.info(f"üìÅ Arquivo criado: {size} bytes")
                else:
                    logger.error(f"‚ùå Arquivo n√£o foi criado: {filename}")
            else:
                logger.warning("‚ö†Ô∏è Nenhum epis√≥dio para salvar no CSV")

        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar CSV: {e}")
            raise

    def run(self, interaction_time=60):
        """Executa todo o processo de scraping"""
        try:
            logger.info("Iniciando scraping do Globo Play...")

            # Setup do driver
            self.setup_driver()

            # Acessa a p√°gina
            logger.info(f"Acessando {self.url}")
            self.driver.get(self.url)

            # Espera p√°gina carregar
            self.wait_for_page_load()

            # No modo visual, aguarda intera√ß√£o do usu√°rio por tempo limitado
            if not self.headless:
                print("\n" + "="*60)
                print("üîÑ MODO INTERATIVO ATIVADO")
                print("üìú INSTRU√á√ïES:")
                print("   1. Use a roda do mouse ou setas para fazer scroll")
                print("   2. Continue fazendo scroll at√© carregar todos os epis√≥dios desejados")
                print("   3. Aguarde - os dados ser√£o extra√≠dos automaticamente")
                print("‚èπÔ∏è  N√ÉO PRECISA FECHAR A JANELA - ela ser√° fechada automaticamente!")
                print("="*60 + "\n")

                # Timer configur√°vel para intera√ß√£o do usu√°rio
                # interaction_time j√° √© passado como par√¢metro do m√©todo
                start_time = time.time()

                episodes_collected = 0
                last_check = time.time()

                print(f"‚è±Ô∏è  Voc√™ tem {interaction_time} segundos para fazer scroll...")
                print("üí° Dica: Fa√ßa scroll na p√°gina para carregar mais epis√≥dios...")

                try:
                    while time.time() - start_time < interaction_time:
                        # Verifica se a janela ainda est√° aberta
                        self.driver.current_url  # Isso lan√ßa uma exce√ß√£o se a janela for fechada

                        # A cada 3 segundos, verifica se h√° novos epis√≥dios
                        current_time = time.time()
                        if current_time - last_check >= 3:
                            try:
                                # Conta epis√≥dios atuais sem salvar (apenas para feedback)
                                current_episodes = len(self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/v/'][href*='?s=0s']"))
                                if current_episodes != episodes_collected:
                                    episodes_collected = current_episodes
                                    elapsed = int(current_time - start_time)
                                    remaining = interaction_time - elapsed
                                    print(f"üìä Epis√≥dios detectados: {episodes_collected} ({remaining}s restantes)")
                                    logger.info(f"Epis√≥dios detectados: {episodes_collected}")
                                last_check = current_time
                            except Exception as count_error:
                                logger.debug(f"Erro ao contar epis√≥dios: {count_error}")

                        time.sleep(1)  # Verifica a cada segundo

                    # Tempo esgotado - extra√ß√£o autom√°tica
                    logger.info("‚è∞ Tempo de intera√ß√£o esgotado - iniciando extra√ß√£o autom√°tica")
                    print(f"üéØ Tempo esgotado! Coletando {episodes_collected} epis√≥dios detectados...")

                except Exception as e:
                    # Janela foi fechada antes do tempo acabar
                    logger.info("‚úÖ Janela do navegador fechada pelo usu√°rio (antes do timer)")
                    print(f"üéØ Janela fechada! Coletando {episodes_collected} epis√≥dios detectados...")
                    # Continua para extra√ß√£o mesmo sem dados coletados

            # Extrai epis√≥dios (seja ap√≥s scroll autom√°tico ou manual)
            logger.info("üîç Iniciando extra√ß√£o de epis√≥dios...")
            self.extract_episodes()
            logger.info(f"üìä Ap√≥s extra√ß√£o: {len(self.episodes)} epis√≥dios encontrados")

            # Salva resultados
            logger.info("üíæ Salvando dados...")
            self.save_to_json()
            self.save_to_csv()

            logger.info("Scraping conclu√≠do com sucesso!")
            logger.info(f"Total de epis√≥dios encontrados: {len(self.episodes)}")

            # Debug final
            print(f"\nüéØ DEBUG FINAL:")
            print(f"   Total de epis√≥dios na lista: {len(self.episodes)}")
            if self.episodes:
                print(f"   Primeiro epis√≥dio: {self.episodes[0]['title'][:50]}...")
                print(f"   Arquivos que deveriam ser criados: {self.json_filename}, {self.csv_filename}")

            return self.episodes

        except Exception as e:
            logger.error(f"Erro durante execu√ß√£o: {e}")
            return []

        finally:
            if self.driver:
                self.driver.quit()

def main():
    """Fun√ß√£o principal"""
    import sys
    import argparse

    # Configura parser de argumentos
    parser = argparse.ArgumentParser(description="Scraper do Globo Play para extrair epis√≥dios")
    parser.add_argument("--url", default="https://globoplay.globo.com/v/13385190/?s=0s",
                       help="URL da p√°gina do Globo Play (padr√£o: epis√≥dio atual)")
    parser.add_argument("--output", default="globoplay_episodes",
                       help="Nome base para os arquivos de sa√≠da (padr√£o: globoplay_episodes)")
    parser.add_argument("--headless", action="store_true",
                       help="Executar em modo headless (sem interface gr√°fica)")
    parser.add_argument("--interaction-time", type=int, default=60,
                       help="Tempo em segundos para intera√ß√£o do usu√°rio no modo visual (padr√£o: 60)")

    args = parser.parse_args()

    # Determina modo de execu√ß√£o
    headless = args.headless
    if headless:
        print("üöÄ Executando em modo HEADLESS (sem interface gr√°fica)")
    else:
        print("üöÄ Executando em modo VISUAL (com interface gr√°fica)")
        print("üí° IMPORTANTE: Fa√ßa scroll MANUALMENTE na p√°gina para carregar mais epis√≥dios!")
        print("   Feche a janela quando terminar de fazer scroll.")

    print(f"üìç URL: {args.url}")
    print(f"üìÅ Output: {args.output}")

    # Cria scraper com par√¢metros
    scraper = GloboPlayScraper(url=args.url, headless=headless)

    # Modifica nomes dos arquivos de sa√≠da
    scraper.json_filename = f"{args.output}.json"
    scraper.csv_filename = f"{args.output}.csv"

    episodes = scraper.run(interaction_time=args.interaction_time)

    # Exibe resumo
    print("\nüìä RESUMO DO SCRAPING:")
    print(f"Total de epis√≥dios encontrados: {len(episodes)}")
    print("Arquivos gerados:")
    print(f"- {args.output}.json")
    print(f"- {args.output}.csv")

    if episodes:
        print("\nüîó PRIMEIROS 10 EPIS√ìDIOS:")
        for i, ep in enumerate(episodes[:10], 1):
            print(f"{i:2d}. {ep['title']} - {ep['url']}")

if __name__ == "__main__":
    main()
